#!/usr/bin/env python3
"""
Image Cache Builder for Crypto Candlesticks (Deterministic, Idempotent, Verifiable)

- Sources OHLCV from your existing market SQLite (read-only, immutable).
- Renders lossless PNG images for fixed-size windows around each bar end time.
- Stores deterministic filenames on disk, and tracks them in a separate index SQLite.
- Supports: render (idempotent), verify (integrity and gap checks), repair (fix issues).
- Designed for accuracy first. Parallelism is conservative; correctness is enforced.

Usage examples:
  python image_cache.py render
  python image_cache.py verify
  python image_cache.py repair
  python image_cache.py render --start 2020-01-01 --end 2025-01-01
  python image_cache.py verify  --symbol BTCUSDT --interval 15m

"""

from __future__ import annotations

import argparse
import hashlib
import os
import sqlite3
import sys
import time
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import mplfinance as mpf
import numpy as np
import pandas as pd

# ===================== CONFIG =====================

# SOURCE: your market data database (read-only)
# This should point to the market.sqlite file generated by the crypto-sqlite-pipeline project.
MARKET_DB_PATH = "../crypto-sqlite-pipeline/market.sqlite"

# TARGET: image index (separate DB) & image root directory
IMAGE_INDEX_DB = "./image_cache.sqlite"
IMAGE_ROOT_DIR = "./image-cache"

# What to build (add more over time as needed)
PAIRS = [
    "BTCUSDT", 
    "DOGEUSDT", 
    "ETHUSDT",  
    "XLMUSDT",
    "XRPUSDT",
    "LTCUSDT",
    "DOTUSDT",
    "SHIBUSDT",
    "BNBUSDT"
    ]
INTERVALS = ["15m", "30m", "1h", "4h", "12h", "1d, 1w"]

# Global range (UTC). You can override via CLI.
DATE_START = "2019-01-01"     # inclusivea
DATE_END   = "2025-08-01"     # exclusive

# Rendering fidelity (accuracy over speed)
IMG_SIZE_PX = 1280            # square
WINDOW_LEN = 150              # candles per image
STYLE_NAME = "mike"        # lock style to ensure deterministic visuals
STYLE_VERSION = 1             # bump when style/padding/size rules change
INCLUDE_VOLUME = False        # keep visuals identical to your backtest
PNG_BBOX_TIGHT = True         # pad-free images (stable layout)

# New visual toggles (global)
DRAW_GRID = False             # grid OFF
AXIS_OFF = False              # keep axes ON (ticks help model anchor)

# Verification policy
HASH_IMAGES = True            # sha256 at creation (and optionally on verify)
VERIFY_HASH_ON_VERIFY = False # set True for deep audit (slower)

# Parallelism (accuracy first; keep modest to avoid flakiness)
MAX_WORKERS = 0               # 0 = single-process (safest determinism)

# ==================================================

INTERVAL_TO_MINUTES = {
    "1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30,
    "1h": 60, "2h": 120, "4h": 240, "6h": 360, "8h": 480, "12h": 720,
    "1d": 1440, "3d": 4320, "1w": 10080,
}

@dataclass(frozen=True)
class Job:
    symbol: str
    interval: str
    month_start_utc: datetime
    month_end_utc: datetime

def log(msg: str) -> None:
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts} UTC] {msg}", flush=True)

def ensure_dirs() -> None:
    Path(IMAGE_ROOT_DIR).mkdir(parents=True, exist_ok=True)
    Path(IMAGE_INDEX_DB).parent.mkdir(parents=True, exist_ok=True)

def parse_date(d: str) -> datetime:
    return datetime.strptime(d, "%Y-%m-%d").replace(tzinfo=timezone.utc)

def to_ms(dt: datetime) -> int:
    return int(dt.timestamp() * 1000)

def floor_to_interval_ms(ts_ms: int, interval_ms: int) -> int:
    return (ts_ms // interval_ms) * interval_ms

@contextmanager
def market_conn():
    # Read-only, immutable to prevent locks
    uri = f"file:{MARKET_DB_PATH}?mode=ro&immutable=1"
    con = sqlite3.connect(uri, uri=True)
    try:
        con.execute("PRAGMA query_only = ON;")
        con.execute("PRAGMA temp_store = MEMORY;")
        con.execute("PRAGMA cache_size = -200000;")
        yield con
    finally:
        con.close()

@contextmanager
def index_conn():
    con = sqlite3.connect(IMAGE_INDEX_DB)
    try:
        con.execute("PRAGMA journal_mode = WAL;")
        con.execute("PRAGMA synchronous = NORMAL;")
        con.execute("PRAGMA foreign_keys = ON;")
        yield con
    finally:
        con.close()

def init_index_schema() -> None:
    ensure_dirs()
    with index_conn() as con:
        con.executescript("""
        CREATE TABLE IF NOT EXISTS images (
            symbol TEXT NOT NULL,
            interval TEXT NOT NULL,
            end_open_time_ms INTEGER NOT NULL,
            img_size_px INTEGER NOT NULL,
            window_len INTEGER NOT NULL,
            style_version INTEGER NOT NULL,
            file_path TEXT NOT NULL,
            sha256 TEXT,
            width_px INTEGER NOT NULL,
            height_px INTEGER NOT NULL,
            created_utc TEXT NOT NULL,
            status TEXT NOT NULL CHECK(status IN ('ok','missing','corrupt','stale')),
            PRIMARY KEY (symbol, interval, end_open_time_ms, img_size_px, window_len, style_version)
        );

        CREATE TABLE IF NOT EXISTS series_meta (
            symbol TEXT NOT NULL,
            interval TEXT NOT NULL,
            start_ms INTEGER,
            end_ms INTEGER,
            bars_expected INTEGER,
            bars_present INTEGER,
            last_scan_utc TEXT,
            PRIMARY KEY(symbol, interval)
        );

        CREATE INDEX IF NOT EXISTS idx_images_lookup
            ON images(symbol, interval, end_open_time_ms);
        """)
        con.commit()

def month_range(start: datetime, end: datetime) -> Iterable[Tuple[datetime, datetime]]:
    # yields [month_start, month_end) pairs
    cur = datetime(start.year, start.month, 1, tzinfo=timezone.utc)
    while cur < end:
        if cur.month == 12:
            nxt = datetime(cur.year + 1, 1, 1, tzinfo=timezone.utc)
        else:
            nxt = datetime(cur.year, cur.month + 1, 1, tzinfo=timezone.utc)
        yield max(cur, start), min(nxt, end)
        cur = nxt

def shard_outdir(symbol: str, interval: str, dt: datetime) -> Path:
    return Path(IMAGE_ROOT_DIR) / symbol / interval / f"{dt.year:04d}" / f"{dt.month:02d}"

def filename(symbol: str, interval: str, end_dt: datetime) -> str:
    # Deterministic: include ISO-ish end time, size, window, style version
    ts = end_dt.strftime("%Y-%m-%dT%H-%M-%S")
    return f"{symbol}_{interval}_{ts}_px{IMG_SIZE_PX}_w{WINDOW_LEN}_v{STYLE_VERSION}.png"

def expected_times_from_market(con: sqlite3.Connection, symbol: str, interval: str,
                               month_start: datetime, month_end: datetime) -> List[int]:
    sql = """
      SELECT c.open_time
      FROM candles c
      JOIN series s   ON s.id = c.series_id
      JOIN symbols sy ON sy.id = s.symbol_id
      JOIN intervals it ON it.id = s.interval_id
      WHERE sy.symbol = ? AND it.code = ? AND c.open_time >= ? AND c.open_time < ?
      ORDER BY c.open_time ASC
    """
    rows = con.execute(sql, (symbol, interval, to_ms(month_start), to_ms(month_end))).fetchall()
    # We want end-of-window bar times; weâ€™ll only render where a full window exists
    return [r[0] for r in rows]

def load_series_for_window(con: sqlite3.Connection, symbol: str, interval: str,
                           end_open_time_ms: int, window_len: int) -> pd.DataFrame:
    # Load window_len bars ending at end_open_time_ms
    sql = """
      SELECT c.open_time, c.open, c.high, c.low, c.close, c.volume
      FROM candles c
      JOIN series s   ON s.id = c.series_id
      JOIN symbols sy ON sy.id = s.symbol_id
      JOIN intervals it ON it.id = s.interval_id
      WHERE sy.symbol = ? AND it.code = ? AND c.open_time <= ?
      ORDER BY c.open_time DESC
      LIMIT ?
    """
    df = pd.read_sql_query(sql, con, params=(symbol, interval, end_open_time_ms, window_len))
    if df.empty or len(df.index) < window_len:
        return pd.DataFrame()
    df = df.iloc[::-1].copy()  # ascending by time
    df["open_time"] = pd.to_datetime(df["open_time"], unit="ms", utc=True).dt.tz_convert(None)
    df.set_index("open_time", inplace=True)
    for col in ["open", "high", "low", "close", "volume"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    return df[["open", "high", "low", "close", "volume"]]

def render_image(window_df: pd.DataFrame, out_path: Path) -> Tuple[int, int]:
    # Build an mpf style from our chosen base style, with explicit grid control.
    # Using rc['axes.grid'] enforces grid ON/OFF deterministically.
    mpf_style = mpf.make_mpf_style(
        base_mpf_style=STYLE_NAME,
        gridstyle="--" if DRAW_GRID else "",
        rc={"axes.grid": DRAW_GRID}
    )

    # Figure size in inches; DPI fixed by backend; size drives pixel resolution deterministically.
    figsize = (IMG_SIZE_PX / 100.0, IMG_SIZE_PX / 100.0)

    fig, axes = mpf.plot(
        window_df,
        type="candle",
        style=mpf_style,
        volume=INCLUDE_VOLUME,
        returnfig=True,
        figsize=figsize,
        warn_too_much_data=1_000_000
    )

    # Optionally remove axes entirely (keeps canvas extents stable due to bbox_tight)
    if AXIS_OFF:
        for ax in (axes if isinstance(axes, (list, tuple)) else [axes]):
            try:
                ax.set_axis_off()
            except Exception:
                pass

    # Save deterministically
    if PNG_BBOX_TIGHT:
        fig.savefig(out_path, format="png", bbox_inches="tight", pad_inches=0.0)
    else:
        fig.savefig(out_path, format="png")

    w, h = fig.canvas.get_width_height()
    plt.close(fig)
    return w, h

def file_sha256(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def upsert_image_row(con: sqlite3.Connection,
                     symbol: str, interval: str, end_ms: int, file_path: Path,
                     width: int, height: int, status: str, sha: Optional[str]) -> None:
    con.execute("""
        INSERT INTO images(symbol, interval, end_open_time_ms, img_size_px, window_len, style_version,
                           file_path, sha256, width_px, height_px, created_utc, status)
        VALUES(?,?,?,?,?,?,?,?,?,?,?,?)
        ON CONFLICT(symbol, interval, end_open_time_ms, img_size_px, window_len, style_version) DO UPDATE SET
           file_path=excluded.file_path,
           sha256=excluded.sha256,
           width_px=excluded.width_px,
           height_px=excluded.height_px,
           created_utc=excluded.created_utc,
           status=excluded.status
    """, (
        symbol, interval, end_ms, IMG_SIZE_PX, WINDOW_LEN, STYLE_VERSION,
        str(file_path), sha, width, height,
        datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
        status
    ))

def get_index_status(con: sqlite3.Connection, symbol: str, interval: str, end_ms: int) -> Optional[Tuple[str, str, int, int, int]]:
    row = con.execute("""
        SELECT status, file_path, width_px, height_px, style_version
        FROM images
        WHERE symbol=? AND interval=? AND end_open_time_ms=? AND img_size_px=? AND window_len=? AND style_version=?
    """, (symbol, interval, end_ms, IMG_SIZE_PX, WINDOW_LEN, STYLE_VERSION)).fetchone()
    return row if row else None

def render_job(symbol: str, interval: str, month_start: datetime, month_end: datetime,
               mode: str) -> Tuple[int, int, int, int]:
    """Returns: (ok, missing, repaired, stale) counts for this shard."""
    ok = missing = repaired = stale = 0
    out_count = 0
    t0 = time.perf_counter()

    with market_conn() as mcon, index_conn() as icon:
        exp_times = expected_times_from_market(mcon, symbol, interval, month_start, month_end)
        if not exp_times:
            return (0, 0, 0, 0)

        # Filter to those with full window
        # Build a set for quick membership and require the smallest time to be >= month_start - window backfill
        interval_ms = INTERVAL_TO_MINUTES[interval] * 60_000
        min_window_start = to_ms(month_start) - (WINDOW_LEN - 1) * interval_ms
        exp_times = [t for t in exp_times if t >= to_ms(month_start) and t - (WINDOW_LEN - 1) * interval_ms >= min_window_start]

        shard_dir = shard_outdir(symbol, interval, month_start)
        shard_dir.mkdir(parents=True, exist_ok=True)

        for i, end_ms in enumerate(exp_times, 1):
            status_row = get_index_status(icon, symbol, interval, end_ms)
            if mode == "render":
                # Skip if already ok and on disk
                if status_row:
                    s, fpath, w, h, sv = status_row
                    if s == "ok" and sv == STYLE_VERSION and Path(fpath).is_file():
                        ok += 1
                        continue
                    # stale/corrupt/missing fall through to render/repair
                # Render
                df = load_series_for_window(mcon, symbol, interval, end_ms, WINDOW_LEN)
                if df.empty:
                    # Source gap: mark missing (cannot fabricate)
                    upsert_image_row(icon, symbol, interval, end_ms, Path(""), 0, 0, "missing", None)
                    missing += 1
                    continue

                end_dt = df.index[-1]
                out_path = shard_dir / filename(symbol, interval, end_dt)
                w, h = render_image(df, out_path)
                sha = file_sha256(out_path) if HASH_IMAGES else None
                upsert_image_row(icon, symbol, interval, end_ms, out_path, w, h, "ok", sha)
                ok += 1
                out_count += 1

            elif mode in ("verify", "repair"):
                need_repair = False
                if not status_row:
                    need_repair = (mode == "repair")
                else:
                    s, fpath, w, h, sv = status_row
                    p = Path(fpath)
                    if s != "ok" or sv != STYLE_VERSION or not p.is_file():
                        need_repair = (mode == "repair")
                    else:
                        # Optional deep hash check
                        if VERIFY_HASH_ON_VERIFY:
                            try:
                                sha_now = file_sha256(p)
                                # If hashes recorded and mismatch => repair
                                rec = icon.execute("SELECT sha256 FROM images WHERE symbol=? AND interval=? AND end_open_time_ms=? AND img_size_px=? AND window_len=? AND style_version=?",
                                                   (symbol, interval, end_ms, IMG_SIZE_PX, WINDOW_LEN, STYLE_VERSION)).fetchone()
                                if rec and rec[0] and rec[0] != sha_now:
                                    need_repair = (mode == "repair")
                            except Exception:
                                need_repair = (mode == "repair")

                if need_repair:
                    # Attempt repair by re-rendering from source
                    df = load_series_for_window(mcon, symbol, interval, end_ms, WINDOW_LEN)
                    if df.empty:
                        upsert_image_row(icon, symbol, interval, end_ms, Path(""), 0, 0, "missing", None)
                        missing += 1
                    else:
                        end_dt = df.index[-1]
                        out_path = shard_dir / filename(symbol, interval, end_dt)
                        w, h = render_image(df, out_path)
                        sha = file_sha256(out_path) if HASH_IMAGES else None
                        upsert_image_row(icon, symbol, interval, end_ms, out_path, w, h, "ok", sha)
                        repaired += 1
                        out_count += 1
                else:
                    ok += 1

            if i % 1000 == 0:
                icon.commit()
                elapsed = time.perf_counter() - t0
                rate = out_count / elapsed if elapsed > 0 else 0.0
                log(f"{mode.upper()} {symbol} {interval} {month_start.strftime('%Y-%m')}: {i}/{len(exp_times)} processed | wrote: {out_count} | {rate:.2f} img/s")

        icon.commit()

    return ok, missing, repaired, stale

def plan_jobs(symbols: List[str], intervals: List[str], start: datetime, end: datetime) -> List[Job]:
    jobs: List[Job] = []
    for sym in symbols:
        for itv in intervals:
            for ms, me in month_range(start, end):
                jobs.append(Job(sym, itv, ms, me))
    return jobs

def main():
    parser = argparse.ArgumentParser(description="Candlestick Image Cache Builder (render/verify/repair)")
    parser.add_argument("mode", choices=["render", "verify", "repair"], help="Operation mode")
    parser.add_argument("--symbol", action="append", help="Filter to specific symbol(s); repeatable")
    parser.add_argument("--interval", action="append", help="Filter to specific interval(s); repeatable")
    parser.add_argument("--start", default=DATE_START, help="UTC start date YYYY-MM-DD (inclusive)")
    parser.add_argument("--end", default=DATE_END, help="UTC end date YYYY-MM-DD (exclusive)")
    args = parser.parse_args()

    init_index_schema()

    symbols = args.symbol if args.symbol else PAIRS
    intervals = args.interval if args.interval else INTERVALS
    start = parse_date(args.start)
    end = parse_date(args.end)

    jobs = plan_jobs(symbols, intervals, start, end)
    log(f"Mode={args.mode} | Symbols={symbols} | Intervals={intervals} | Range={args.start}â†’{args.end} | Jobs={len(jobs)}")

    total_ok = total_missing = total_repaired = total_stale = 0
    t0_all = time.perf_counter()

    # Single-process for maximum determinism and simple logging.
    for j in jobs:
        ok, missing, repaired, stale = render_job(j.symbol, j.interval, j.month_start_utc, j.month_end_utc, args.mode)
        total_ok += ok
        total_missing += missing
        total_repaired += repaired
        total_stale += stale
        log(f"Shard {j.symbol} {j.interval} {j.month_start_utc.strftime('%Y-%m')}: ok={ok} missing={missing} repaired={repaired} stale={stale}")

    elapsed = time.perf_counter() - t0_all
    log(f"DONE {args.mode.upper()}: ok={total_ok} missing={total_missing} repaired={total_repaired} stale={total_stale} | elapsed={elapsed:.1f}s")

if __name__ == "__main__":
    main()
